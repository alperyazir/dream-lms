# Story 27.2: DeepSeek Provider Integration

**Status:** Ready for Review

**Epic:** Epic 27 - DreamAI - AI-Powered Content Generation
**Story Points:** 3
**Priority:** High
**Dependencies:** Story 27.1 (LLM Provider Abstraction Layer)

---

## Story

**As a** system,
**I want** DeepSeek implemented as the primary LLM provider,
**so that** content generation uses a cost-effective, high-quality AI model.

---

## Acceptance Criteria

1. [x] DeepSeek API client implementation
2. [x] Support for DeepSeek-V3 model (deepseek-chat)
3. [x] Structured JSON output support
4. [x] Error handling and retry logic
5. [x] Cost tracking per request
6. [x] Environment-based API key configuration

---

## Tasks / Subtasks

- [x] **Task 1: Create DeepSeek Provider Class** (AC: 1, 2, 6)
  - [x] Create `backend/app/services/llm/providers/deepseek.py`
  - [x] Implement `DeepSeekProvider` extending `LLMProvider`
  - [x] Configure API endpoint: `https://api.deepseek.com/v1/chat/completions`
  - [x] Use model: `deepseek-chat` (V3)
  - [x] Load API key from `DEEPSEEK_API_KEY` environment variable
  - [x] Implement `is_available()` to check API key presence

- [x] **Task 2: Implement Generate Method** (AC: 1, 5)
  - [x] Implement async `generate()` method
  - [x] Build request payload with prompt and options
  - [x] Parse response and extract content
  - [x] Track token usage from response
  - [x] Calculate cost using TokenUsage.calculate_cost()
  - [x] Measure and return latency_ms

- [x] **Task 3: Implement Structured Generation** (AC: 3)
  - [x] Implement async `generate_structured()` method
  - [x] Use JSON mode in API request
  - [x] Parse JSON response
  - [x] Validate response against schema (basic validation)
  - [x] Handle JSON parsing errors gracefully

- [x] **Task 4: Implement Error Handling and Retry** (AC: 4)
  - [x] Map DeepSeek API errors to LLMProviderError hierarchy:
    - 401 → LLMAuthenticationError
    - 429 → LLMRateLimitError (with retry_after)
    - 500/502/503 → LLMConnectionError
    - Timeout → LLMTimeoutError
  - [x] Implement retry with exponential backoff for transient errors
  - [x] Use settings for max retries and timeout

- [x] **Task 5: Register Provider with Manager** (AC: 1)
  - [x] Update `__init__.py` to export DeepSeekProvider
  - [x] Add factory function to create and register provider

- [x] **Task 6: Write Unit Tests** (AC: All)
  - [x] Test provider initialization
  - [x] Test generate with mocked httpx response
  - [x] Test generate_structured with mocked response
  - [x] Test error mapping for different HTTP status codes
  - [x] Test retry logic
  - [x] Test token usage calculation

---

## Dev Notes

### API Reference
- **Endpoint:** `https://api.deepseek.com/v1/chat/completions`
- **Model:** `deepseek-chat` (DeepSeek-V3)
- **Auth:** Bearer token via `Authorization` header
- **Compatible with OpenAI API format**

### Request Format
```json
{
  "model": "deepseek-chat",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
  ],
  "temperature": 0.7,
  "max_tokens": 2000,
  "response_format": {"type": "json_object"}  // For structured output
}
```

### Response Format
```json
{
  "id": "chatcmpl-xxx",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "deepseek-chat",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 20,
    "total_tokens": 30
  }
}
```

### Cost Tracking
- Input: $0.14 per 1M tokens
- Output: $0.28 per 1M tokens
- Already implemented in `TokenUsage.calculate_cost()`

### Source Tree Reference
```
backend/app/services/llm/
├── providers/
│   ├── __init__.py
│   └── deepseek.py      # NEW
└── ...
```

---

## Testing

### Test File Location
`backend/app/tests/test_services/test_llm/test_deepseek.py`

### Test Standards
- Use `pytest` with `pytest-asyncio`
- Mock `httpx.AsyncClient` for API calls
- Use `respx` or manual mocking for HTTP responses

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-01-02 | 0.1 | Initial story draft | Bob (SM Agent) |
| 2026-01-02 | 0.2 | Implementation complete | James (Dev Agent) |

---

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References
None - implementation proceeded without issues.

### Completion Notes List
- Implemented DeepSeekProvider class extending the abstract LLMProvider
- Uses httpx AsyncClient for async HTTP requests to DeepSeek API
- Full support for DeepSeek-V3 model (deepseek-chat) with OpenAI-compatible API
- Implemented both `generate()` and `generate_structured()` methods
- Error handling maps API errors to LLMProviderError hierarchy (401→Auth, 429→RateLimit, 5xx→Connection)
- Retry logic with exponential backoff for transient errors (timeout, connection, rate limit)
- Cost tracking using TokenUsage.calculate_cost() with DeepSeek pricing
- Added create_default_manager() factory function for provider registration
- All 24 unit tests passing with comprehensive coverage

### File List
**Created:**
- `backend/app/services/llm/providers/__init__.py` - Provider package exports
- `backend/app/services/llm/providers/deepseek.py` - DeepSeekProvider implementation
- `backend/app/tests/test_services/test_llm/test_deepseek.py` - Unit tests (24 tests)

**Modified:**
- `backend/app/services/llm/__init__.py` - Added DeepSeekProvider and create_default_manager exports
- `backend/app/services/llm/manager.py` - Added create_default_manager() factory function

---

## QA Results
_To be filled by QA agent_
